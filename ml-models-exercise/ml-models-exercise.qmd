---
title: "Machine Learning Models Exercise"
author: "Emma Hardin-Parker"
date: "3/26/2024"
---

# Introductory Processes 

Loading required packages
```{r message = FALSE}
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(GGally)
library(corrplot) 
library(glmnet)
library(ranger)
library(yardstick) 
library(here)
```

Defining a random number seed 
```{r}
rngseed = 1234
```

Loading the data
```{r}
# Specify the path to your RDS file
rds_file <- "/Users/ehardinparker/Desktop/Completed/CPH_Spring24/BIOS8060E/emmahardinparker-MADA-portfolio/ml-models-exercise/data_with_race.rds"

# Use readRDS() to load the data from the RDS file
race_data <- readRDS(here(rds_file))

```

# Futher Processing

The RACE variable in this data set is coded arbitrarily. I am going to attempt to find out what the codes 88 and 7 represent in this dataset. 

```{r}
# Visualizing count data of the RACE variable 
countplot <- ggplot(race_data, aes(x = RACE)) + geom_bar()
print(countplot)
```
Now that I have confirmed that codes 88 and 7 have the fewest observations in the data, I want to see if any other data metric is useful in determining the codes. 

```{r}
# 88
eightyeight <- race_data %>%
  filter(RACE == "88")
summary(eightyeight)
```
```{r}
# 7 
seven <- race_data %>%
  filter(RACE == "7")
summary(seven)
```
Here we can see that those with the RACE code 7 weigh approximately 8.3 kg more than those with the RACE code 88. However, it is important to note that there are 8 individuals with the code 88 and only 2 individuals with the code 7. Similarly, those with the RACE code 7 are shorter, on average, than those with the RACE variable 88. 

Plotting the different Heights and Weights of RACE variable codes.

```{r}
ht_vs_wt <- ggplot(race_data, aes(x = HT, y = WT, color = factor(RACE))) +
  geom_point() +
  labs(title = "Height vs. Weight by Race",
       x = "Height",
       y = "Weight",
       color = "Race") +
  scale_color_discrete(name = "Race", labels = c("1", "2", "7", "88")) +
  theme_minimal()
print(ht_vs_wt)
```
From these tibbles and plots, I am inferring that RACE codes 1 and 2 represent "white" and "non-white" as the paper dictates that most of the observations were taken from Caucasian males. Since RACE code 7 has the least amount of observations, I assume this code is for missing data. Along the same lines, I would assume that if 1 and 2 are binary for "white" and "non-white", RACE code 88 would indicated mixed race. 


I am now going to recode the RACE variable so categories 88 and 7 are combined into a new category, 3. 
```{r}
recoded <- race_data %>%
  mutate(RACE = case_when(RACE == "1" ~ 1,
                          RACE == "2" ~ 2,
                          RACE == "88" ~ 3,
                          RACE == "7" ~ 3))
```

# Pairwise Correlation 

Next, I am going to make a pairwise correlation plot for the continuous variables (not including RACE and SEX). If strong correlations are found ( > 0.9) I may elect to remove them to bypass the potential issue of collinearity. 


```{r}
# Filter only continuous variables from the dataset
continuous_vars <- recoded %>%
  select(-SEX, -RACE)

# Create a correlation matrix for continuous variables in recoded
cor_matrix <- cor(continuous_vars, use = "pairwise.complete.obs")

# Visualize the correlation matrix using corrplot
corrplot(cor_matrix, method = "circle", type = "upper", tl.cex = 0.7)

```
Height and Weight have the strongest correlation with a coefficient of approximately 0.6. There is no evidence of robust correlation among these variables (coefficient > 0.9). Therefore, I won't need to remove any variables from the data annd I can move on. 

### Feature Engineering

Now I am going to further recode the data to combine HT and WT into a singular variable, BMI, using the following [formula](https://www.cdc.gov/healthyweight/assessing/bmi/childrens_BMI/childrens_BMI_formula.html#:~:text=Formula%20and%20Calculation&text=The%20formula%20for%20BMI%20is,by%20height%20in%20meters%20squared.) I am assuming that the HT and WT coded in the data are reflective of HT in meters and WT in kilograms. 

```{r}
eng_data <- recoded %>%
  mutate(BMI = (WT / (HT)^2))
```

# Model Building

I am now going to explore the following models: 

1. Linear model with all predictors
2. LASSO Regression
3. Random Forest 

## First Model: Linear model with all predictors 

```{r}
# Set seed 
set.seed(rngseed)

# Linear regression model
linear_model <- linear_reg() %>% set_engine("lm")

# Fitted model
all_model <- linear_model %>% fit(Y ~., data = eng_data)

# Define the outcome variable (Y) and predictors (all other variables)
outcome <- "Y"
predictors <- setdiff(names(eng_data), outcome)

# Create a recipe for preprocessing
data_recipe <- recipe(formula = Y ~., data = eng_data) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_normalize(all_predictors())

# Linear workflow 

linear_wf <- workflow() %>%
  add_recipe(data_recipe) %>%
  add_model(linear_model)

```


## Second Model: LASSO Regression 

```{r}
# LASSO regression model
lasso_model <- linear_reg(penalty = 0.1, mixture = 1) %>%
  set_engine("glmnet")

# LASSO workflow
lasso_wf <- workflow() %>%
  add_recipe(data_recipe) %>%
  add_model(lasso_model)

```


## Third Model: Random Forest (RF)

```{r}
# Random Forest Model
rf_model <- rand_forest() %>%
  set_engine("ranger", seed = rngseed) %>%
  set_mode("regression")

# Random Forest Workflow 
rf_workflow <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(data_recipe)
```

## Fitting all three Models

```{r}
all_fit <- linear_wf %>% #Linear Model Fit
  fit(data = eng_data)

lasso_fit <- lasso_wf %>% #LASSO Model Fit
  fit(data = eng_data)

rf_fit <- rf_workflow %>% #RF Model Fit
  fit(data = eng_data)
```

## Make predictions for each model and calculate RMSE 

```{r}
# Predictions
all_predictions <- predict(all_fit, new_data = eng_data) 
lasso_predictions <- predict(lasso_fit, new_data = eng_data)
rf_predictions <- predict(rf_fit, new_data = eng_data)

# RMSE
all_rmse <- augment_all <- augment(all_fit, eng_data) %>%
  select(Y, .pred) %>%
  rmse(truth = Y, .pred)
lasso_rmse <- augment_lasso <- augment(lasso_fit, eng_data) %>%
  select(Y, .pred) %>%
  rmse(truth = Y, .pred)
rf_rmse <- augment_rf <- augment(rf_fit, eng_data) %>%
  select(Y, .pred) %>%
  rmse(truth = Y, .pred)


# Printing RMSE 
print(paste("Linear Model RMSE:", all_rmse))
print(paste("Lasso Model RMSE:", lasso_rmse))
print(paste("Random Forest RMSE:", rf_rmse))
  
```

The Linear Model and LASSO Model yielded similar RMSE results (581.42 & 581.47). This may have occurred due to our data set having a small number of predictors and these predictors showing low levels of multicollinearity (the predictors are not highly correlated). Since we are observing a low level of multicollinearity, the LASSO model may not eliminate predictors due to nonzero coefficients. The RF Model yielded an RMSE value of 357.97, rendering it the best performing model so far. This makes sense due to the fact that Random Forest Models are flexible and  can adapt well to various types of data without requiring strict assumptions about the underlying data distribution. This flexibility allows them to perform well in a wide range of scenarios (non-linearity, overfitting, outliers, etc.).

## Observed vs. Predicted Plots for each of the models 

I am now going to create observed vs. predicted plots for each model. 

```{r}
# Plot observed versus predicted for the linear model
linear_plot <- ggplot(data.frame(Observed = eng_data$Y, Predicted = all_predictions$.pred), aes(x = Observed, y = Predicted)) +
  geom_point(color = "blue") +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  labs(title = "Observed vs Predicted - Linear Model",
       x = "Observed",
       y = "Predicted")

# Show the plot
print(linear_plot)

# Plot observed versus predicted for the LASSO model
lasso_plot <- ggplot(data.frame(Observed = eng_data$Y, Predicted = lasso_predictions$.pred), aes(x = Observed, y = Predicted)) +
  geom_point(color = "green") +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  labs(title = "Observed vs Predicted - LASSO Model",
       x = "Observed",
       y = "Predicted")

# Show the plot
print(lasso_plot)


# Plot observed versus predicted for the random forest model
rf_plot <- ggplot(data.frame(Observed = eng_data$Y, Predicted = rf_predictions$.pred), aes(x = Observed, y = Predicted)) +
  geom_point(color = "purple") +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  labs(title = "Observed vs Predicted - Random Forest Model",
       x = "Observed",
       y = "Predicted")

# Show the plot
print(rf_plot)

```
We can see from these plots that once again, the Random Forest Model performed the best when visualizing observed vs. predicted values. The points on the RF Model plot are closest to the abline, indicating a higher level of accuracy than the Linear and LASSO models. This is on par with what we have seen with the previously calculated RMSE values. 

